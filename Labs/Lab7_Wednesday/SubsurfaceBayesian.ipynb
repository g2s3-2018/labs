{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\def\\data{ {\\bf d}_\\rm{obs}}\n",
    "\\def\\vec{\\bf}\n",
    "\\def\\m{ {\\bf m}}\n",
    "\\def\\map{m_{\\nu}}\n",
    "\\def\\postcov{ \\mathcal{C}_{\\text{post}} }\n",
    "\\def\\prcov{ \\mathcal{C}_{\\text{prior}} }\n",
    "\\def\\matrix{\\bf}\n",
    "\\def\\Hmisfit{ \\mathcal{H}_{\\text{misfit}} }\n",
    "\\def\\diag{\\operatorname{diag}}\n",
    "\\def\\Vr{{\\matrix V}_r}\n",
    "\\def\\Wr{{\\matrix W}_r}\n",
    "\\def\\Ir{{\\matrix I}_r}\n",
    "\\def\\Dr{{\\matrix D}_r}\n",
    "\\def\\H{{\\matrix H} }\n",
    "$$ \n",
    "# Bayesian quantification of parameter uncertainty:\n",
    "## Estimating the Gaussian approximation of posterior pdf of the coefficient parameter field in an elliptic PDE\n",
    "\n",
    "In this example we tackle the problem of quantifying the\n",
    "uncertainty in the solution of an inverse problem governed by an\n",
    "elliptic PDE via the Bayesian inference framework. \n",
    "Hence, we state the inverse problem as a\n",
    "problem of statistical inference over the space of uncertain\n",
    "parameters, which are to be inferred from data and a physical\n",
    "model.  The resulting solution to the statistical inverse problem\n",
    "is a posterior distribution that assigns to any candidate set of\n",
    "parameter fields our belief (expressed as a probability) that a\n",
    "member of this candidate set is the ``true'' parameter field that\n",
    "gave rise to the observed data.\n",
    "\n",
    "\n",
    "### Bayes's Theorem\n",
    "\n",
    "The posterior probability distribution combines the prior pdf\n",
    "$\\mu_{\\text{prior}}(m)$ over the parameter space, which encodes\n",
    "any knowledge or assumptions about the parameter space that we may\n",
    "wish to impose before the data are considered, with a likelihood pdf\n",
    "$\\pi_{\\text{like}}(\\data \\; | \\; m)$, which explicitly\n",
    "represents the probability that a given parameter $m$\n",
    "might give rise to the observed data $\\data \\in\n",
    "\\mathbb{R}^{n_t}$, namely:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "d \\mu_{\\text{post}}(m | \\data) \\propto \\pi_{\\text{like}}(\\data \\,|\\, m) \\, d\\mu_{\\text{prior}}(m).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that infinite-dimensional analog of Bayes's formula requires the use Radon-Nikodym derivatives instead of probability density functions.\n",
    "\n",
    "### Gaussian prior and noise\n",
    "\n",
    "#### The prior\n",
    "\n",
    "We consider a Gaussian prior with mean ${m}_{\\rm prior}$ and covariance $\\prcov$, $\\mu_{\\rm prior} \\sim \\mathcal{N}({m}_{\\rm prior}, \\prcov)$. The covariance is given by the discretization of the inverse of differential operator $\\mathcal{A}^{-2} = (-\\gamma \\Delta + \\delta I)^{-2}$, where $\\gamma$, $\\delta > 0$ control the correlation length and the variance of the prior operator. This choice of prior ensures that it is a trace-class operator, guaranteeing bounded pointwise variance and a well-posed infinite-dimensional Bayesian inverse problem.\n",
    "\n",
    "#### The likelihood\n",
    "\n",
    "$$\n",
    "\\data =  {\\bf f}(m) + {\\bf e }, \\;\\;\\;  {\\bf e} \\sim \\mathcal{N}({\\bf 0}, {\\bf \\Gamma}_{\\text{noise}} )\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi_{\\text like}(\\data \\; | \\; m)  = \\exp \\left( - \\tfrac{1}{2} \\parallel {\\bf f}(m) - \\data \\parallel^{2}_{{\\bf \\Gamma}_{\\text{noise}}^{-1}}\\right)\n",
    "$$\n",
    "\n",
    "Here ${\\bf f}$ is the parameter-to-observable map that takes a parameter $m$ and maps\n",
    "it to the space observation vector $\\data$.\n",
    "\n",
    "In this application, ${\\bf f}$ consists in the composition of a PDE solve (to compute the state $u$) and a pointwise observation of the state $u$ to extract the observation vector $\\data$.\n",
    "\n",
    "#### The posterior\n",
    "\n",
    "$$\n",
    "d\\mu_{\\text{post}}(m \\; | \\; \\data)  \\propto \\exp \\left( - \\tfrac{1}{2} \\parallel {\\bf f}(m) - \\data \\parallel^{2}_{{\\bf \\Gamma}_{\\text{noise}}^{-1}} \\! - \\tfrac{1}{2}\\parallel m - m_{\\rm prior} \\parallel^{2}_{\\prcov^{-1}} \\right)\n",
    "$$\n",
    "\n",
    "### The Laplace approximation to the posterior: $\\nu \\sim \\mathcal{N}({\\map},\\bf \\postcov)$\n",
    "\n",
    "The mean of the Laplace approximation posterior distribution, ${\\map}$, is the\n",
    "parameter maximizing the posterior, and\n",
    "is known as the maximum a posteriori (MAP) point.  It can be found\n",
    "by minimizing the negative log of the posterior, which amounts to\n",
    "solving a deterministic inverse problem) with appropriately weighted norms,\n",
    "\n",
    "$$\n",
    "\\map := \\underset{m}{\\arg \\min} \\; \\mathcal{J}(m) \\;:=\\;\n",
    "\\Big( \n",
    "\\frac{1}{2} \\| {\\bf f}(m) - \\data \\|^2_{ {\\bf \\Gamma}_{\\text{noise}}^{-1}} \n",
    "+\\frac{1}{2} \\| m -m_{\\rm prior} \\|^2_{\\prcov^{-1}} \n",
    "\\Big).\n",
    "$$\n",
    "\n",
    "The posterior covariance matrix is then given by the inverse of\n",
    "the Hessian matrix of $\\mathcal{J}$ at $\\map$, namely\n",
    "\n",
    "$$\n",
    "\\postcov = \\left(\\Hmisfit(\\map) + \\prcov^{-1} \\right)^{-1},\n",
    "$$\n",
    "\n",
    "provided that $\\Hmisfit(\\map)$ is positive definite.\n",
    "\n",
    "\n",
    "#### The generalized eigenvalue problem\n",
    "\n",
    "$$\n",
    "\\def\\matHmis{ {\\H}_{\\rm misfit}}\n",
    "\\def\\Gpost{\\boldsymbol{\\Gamma}_{\\rm post} }\n",
    "\\def\\Gprior{ \\boldsymbol{\\Gamma}_{\\rm prior} }\n",
    "$$\n",
    "\n",
    "In what follows we denote with $\\matHmis, \\Gpost, \\Gprior \\in \\mathbb{R}^{n\\times n}$ the matrices stemming from the discretization of the operators $\\Hmisfit(\\map)$, $\\postcov$, $\\prcov$ with respect to the unweighted Euclidean inner product.\n",
    "Then we considered the symmetric generalized eigenvalue problem\n",
    "\n",
    "$$\n",
    " \\matHmis {\\matrix V} = \\Gprior^{-1} {\\matrix V} {\\matrix \\Lambda},\n",
    "$$\n",
    "\n",
    "where ${\\matrix \\Lambda} = \\diag(\\lambda_i) \\in \\mathbb{R}^{n\\times n}$\n",
    "contains the generalized eigenvalues and the columns of ${\\matrix V}\\in\n",
    "\\mathbb R^{n\\times n}$ the generalized eigenvectors such that \n",
    "${\\matrix V}^T \\prcov^{-1} {\\matrix V} = {\\matrix I}$.\n",
    "\n",
    "#### Randomized eigensolvers to construct the approximate spectral decomposition\n",
    "\n",
    "When the generalized eigenvalues $\\{\\lambda_i\\}$ decay rapidly, we can\n",
    "extract a low-rank approximation of $\\matHmis$ by retaining only the $r$\n",
    "largest eigenvalues and corresponding eigenvectors,\n",
    "\n",
    "$$\n",
    " \\matHmis \\approx \\Gprior^{-1} \\Vr {\\matrix{\\Lambda}}_r \\Vr^T \\Gprior^{-1},\n",
    "$$\n",
    "\n",
    "Here, $\\Vr \\in \\mathbb{R}^{n\\times r}$ contains only the $r$\n",
    "generalized eigenvectors of $\\matHmis$ that correspond to the $r$ largest eigenvalues,\n",
    "which are assembled into the diagonal matrix ${\\matrix{\\Lambda}}_r = \\diag\n",
    "(\\lambda_i) \\in \\mathbb{R}^{r \\times r}$.\n",
    "\n",
    "#### The approximate posterior covariance\n",
    "\n",
    "Using the Sherman–Morrison–Woodbury formula, we write\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\notag \\Gpost = \\left(\\matHmis+ \\Gprior^{-1}\\right)^{-1}\n",
    "  = \\Gprior^{-1}-\\Vr {\\matrix{D}}_r \\Vr^T +\n",
    "  \\mathcal{O}\\left(\\sum_{i=r+1}^{n} \\frac{\\lambda_i}{\\lambda_i +\n",
    "    1}\\right),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where ${\\matrix{D}}_r :=\\diag(\\lambda_i/(\\lambda_i+1)) \\in\n",
    "\\mathbb{R}^{r\\times r}$. The last term in this expression captures the\n",
    "error due to truncation in terms of the discarded eigenvalues; this\n",
    "provides a criterion for truncating the spectrum, namely that $r$ is\n",
    "chosen such that $\\lambda_r$ is small relative to 1. \n",
    "\n",
    "Therefore we can approximate the posterior covariance as\n",
    "\n",
    "$$\n",
    "\\Gpost \\approx \\Gprior - \\Vr {\\matrix{D}}_r \\Vr^T\n",
    "$$\n",
    "\n",
    "#### Drawing samples from a Gaussian distribution with covariance $\\Gpost$\n",
    "\n",
    "Let ${\\bf x}$ be a sample for the prior distribution, i.e. ${\\bf x} \\sim \\mathcal{N}({\\bf 0}, \\Gprior)$, then, using the low rank approximation of the posterior covariance, we compute a sample ${\\bf v} \\sim \\mathcal{N}({\\bf 0}, \\Gpost)$ as\n",
    "\n",
    "$$\n",
    "  {\\bf v} = \\big\\{ \\Vr \\big[ ({\\matrix{\\Lambda}}_r +\n",
    "    \\Ir)^{-1/2} - \\Ir \\big] \\Vr^T\\Gprior^{-1}  + {\\bf I} \\big\\} {\\bf x} \n",
    "$$\n",
    "\n",
    "## This tutorial shows:\n",
    "\n",
    "- Description of the inverse problem (the forward problem, the prior, and the misfit functional)\n",
    "- Convergence of the inexact Newton-CG algorithm\n",
    "- Low-rank-based approximation of the posterior covariance (built on a low-rank\n",
    "approximation of the Hessian of the data misfit) \n",
    "- How to construct the low-rank approximation of the Hessian of the data misfit\n",
    "- How to apply the inverse and square-root inverse Hessian to a vector efficiently\n",
    "- Samples from the Gaussian approximation of the posterior\n",
    "\n",
    "## Goals:\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "\n",
    "- Understand the Bayesian inverse framework\n",
    "- Visualise and understand the results\n",
    "- Modify the problem and code\n",
    "\n",
    "## Mathematical tools used:\n",
    "\n",
    "- Finite element method\n",
    "- Derivation of gradiant and Hessian via the adjoint method\n",
    "- inexact Newton-CG\n",
    "- Armijo line search\n",
    "- Bayes' formula\n",
    "- randomized eigensolvers\n",
    "\n",
    "## List of software used:\n",
    "\n",
    "- <a href=\"http://fenicsproject.org/\">FEniCS</a>, a parallel finite element element library for the discretization of partial differential equations\n",
    "- <a href=\"http://www.mcs.anl.gov/petsc/\">PETSc</a>, for scalable and efficient linear algebra operations and solvers\n",
    "- <a href=\"http://matplotlib.org/\">Matplotlib</a>, A great python package that I used for plotting many of the results\n",
    "- <a href=\"http://www.numpy.org/\">Numpy</a>, A python package for linear algebra.  While extensive, this is mostly used to compute means and sums in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import dolfin as dl\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from hippylib import *\n",
    "\n",
    "import logging\n",
    "logging.getLogger('FFC').setLevel(logging.WARNING)\n",
    "logging.getLogger('UFL').setLevel(logging.WARNING)\n",
    "dl.set_log_active(False)\n",
    "\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate the true parameter\n",
    "\n",
    "This function generates a random field with a prescribed anysotropic covariance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_model(prior):\n",
    "    noise = dl.Vector()\n",
    "    prior.init_vector(noise,\"noise\")\n",
    "    parRandom.normal(1., noise)\n",
    "    mtrue = dl.Vector()\n",
    "    prior.init_vector(mtrue, 0)\n",
    "    prior.sample(noise,mtrue)\n",
    "    return mtrue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set up the mesh and finite element spaces\n",
    "\n",
    "We compute a two dimensional mesh of a unit square with nx by ny elements.\n",
    "We define a P2 finite element space for the *state* and *adjoint* variable and P1 for the *parameter*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 2\n",
    "nx = 32\n",
    "ny = 32\n",
    "mesh = dl.UnitSquareMesh(nx, ny)\n",
    "Vh2 = dl.FunctionSpace(mesh, 'Lagrange', 2)\n",
    "Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\n",
    "Vh = [Vh2, Vh1, Vh2]\n",
    "print( \"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format(\n",
    "    Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set up the forward problem\n",
    "\n",
    "Let $\\Omega$ be the unit square in $\\mathbb{R}^2$, and $\\Gamma_D$, $\\Gamma_N$  be the Dirichlet and Neumann portitions of the boundary $\\partial \\Omega$ (that is $\\Gamma_D \\cup \\Gamma_N = \\partial \\Omega$, $\\Gamma_D \\cap \\Gamma_N = \\emptyset$). The forward problem reads\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\nabla \\cdot \\left( e^m \\nabla u\\right) = f & \\text{in } \\Omega\\\\\n",
    "u = u_D & \\text{on } \\Gamma_D, \\\\\n",
    "e^m \\nabla u \\cdot \\boldsymbol{n} = 0 & \\text{on } \\Gamma_N,\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "where $u \\in \\mathcal{V}$ is the state variable, and $m \\in \\mathcal{M}$ is the uncertain parameter. Here $\\Gamma_D$ corresponds to the top and bottom sides of the unit square, and $\\Gamma_N$ corresponds to the left and right sides.\n",
    "We also let $f = 0$, and $u_D = 1$ on the top boundary and $u_D = 0$ on the bottom boundary.\n",
    "\n",
    "To set up the forward problem we use the `PDEVariationalProblem` class, which requires the following inputs\n",
    "- the finite element spaces for the state, parameter, and adjoint variables `Vh`\n",
    "- the pde in weak form `pde_varf`\n",
    "- the boundary conditions `bc` for the forward problem and `bc0` for the adjoint and incremental problems.\n",
    "\n",
    "The `PDEVariationalProblem` class offer the following functionality:\n",
    "- solving the forward/adjoint and incremental problems\n",
    "- evaluate first and second partial derivative of the forward problem with respect to the state, parameter, and adojnt variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u_boundary(x, on_boundary):\n",
    "    return on_boundary and ( x[1] < dl.DOLFIN_EPS or x[1] > 1.0 - dl.DOLFIN_EPS)\n",
    "\n",
    "u_bdr = dl.Expression(\"x[1]\", degree=1)\n",
    "u_bdr0 = dl.Constant(0.0)\n",
    "bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\n",
    "bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)\n",
    "\n",
    "f = dl.Constant(0.0)\n",
    "    \n",
    "def pde_varf(u,m,p):\n",
    "    return dl.exp(m)*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx - f*p*dl.dx\n",
    "    \n",
    "pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set up the prior\n",
    "\n",
    "To obtain the synthetic true paramter $m_{\\rm true}$ we generate a realization from the prior distribution.\n",
    "\n",
    "Here we assume a Gaussian prior, $\\mu_{\\rm prior} \\sim \\mathcal{N}(0, \\prcov)$ with zero mean and covariance matrix $\\prcov = \\mathcal{A}^{-2}$, where $\\mathcal{A}$ is a differential operator of the form\n",
    "\n",
    "$$ \\mathcal{A} = \\gamma {\\rm div}\\, \\Theta\\, {\\rm grad} + \\delta I. $$\n",
    "\n",
    "Here $\\Theta$ is an s.p.d. anisotropic tensor of the form\n",
    "\n",
    "$$ \\Theta =\n",
    "\\begin{bmatrix}\n",
    "\\theta_1 \\sin(\\alpha)^2 & (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} \\\\\n",
    "(\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} & \\theta_2 \\cos(\\alpha)^2.\n",
    "\\end{bmatrix}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = .1\n",
    "delta = .5\n",
    "    \n",
    "anis_diff = dl.Expression(code_AnisTensor2D, degree=1)\n",
    "anis_diff.theta0 = 2.\n",
    "anis_diff.theta1 = .5\n",
    "anis_diff.alpha = math.pi/4\n",
    "prior = BiLaplacianPrior(Vh[PARAMETER], gamma, delta, anis_diff)\n",
    "print(\"Prior regularization: (delta_x - gamma*Laplacian)^order: delta={0}, gamma={1}, order={2}\".format(delta, gamma,2))    \n",
    "\n",
    "\n",
    "mtrue = true_model(prior)\n",
    "                   \n",
    "objs = [dl.Function(Vh[PARAMETER],mtrue), dl.Function(Vh[PARAMETER],prior.mean)]\n",
    "mytitles = [\"True Parameter\", \"Prior mean\"]\n",
    "nb.multi1_plot(objs, mytitles)\n",
    "plt.show()\n",
    "\n",
    "model = Model(pde,prior, misfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Set up the misfit functional and generate synthetic observations\n",
    "\n",
    "To setup the observation operator $\\mathcal{B}: \\mathcal{V} \\mapsto \\mathbb{R}^{n_t}$, we generate $n_t$ (`ntargets` in the code below) random locations where to evaluate the value of the state.\n",
    "\n",
    "Under the assumption of Gaussian additive noise, the likelihood function $\\pi_{\\rm like}$ has the form\n",
    "\n",
    "$$\\pi_{\\rm like}( \\data \\,| \\, m ) \\propto \\exp\\left( -\\frac{1}{2}\\|\\mathcal{B}\\,u(m) - \\data \\|^2_{\\Gamma_{\\rm noise}^{-1}}\\right), $$\n",
    "\n",
    "where $u(m)$ denotes the solution of the forward model at a given parameter $m$.\n",
    "\n",
    "The class `PointwiseStateObservation` implements the evaluation of the log-likelihood function and of its partial derivatives w.r.t. the state $u$ and parameter $m$.\n",
    "\n",
    "To generate the synthetic observation, we first solve the forward problem using the true parameter $m_{\\rm true}$. Synthetic observations are obtained by perturbing the state variable at the observation points with a random Gaussian noise.\n",
    "`rel_noise` is the signal to noise ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntargets = 300\n",
    "rel_noise = 0.005\n",
    "\n",
    "\n",
    "targets = np.random.uniform(0.05,0.95, [ntargets, ndim] )\n",
    "print( \"Number of observation points: {0}\".format(ntargets) )\n",
    "misfit = PointwiseStateObservation(Vh[STATE], targets)\n",
    "\n",
    "utrue = pde.generate_state()\n",
    "x = [utrue, mtrue, None]\n",
    "pde.solveFwd(x[STATE], x, 1e-9)\n",
    "misfit.B.mult(x[STATE], misfit.d)\n",
    "MAX = misfit.d.norm(\"linf\")\n",
    "noise_std_dev = rel_noise * MAX\n",
    "parRandom.normal_perturb(noise_std_dev, misfit.d)\n",
    "misfit.noise_variance = noise_std_dev*noise_std_dev\n",
    "\n",
    "vmax = max( utrue.max(), misfit.d.max() )\n",
    "vmin = min( utrue.min(), misfit.d.min() )\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True State\", subplot_loc=121, vmin=vmin, vmax=vmax)\n",
    "nb.plot_pts(targets, misfit.d, mytitle=\"Observations\", subplot_loc=122, vmin=vmin, vmax=vmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set up the model and test gradient and Hessian\n",
    "\n",
    "The model is defined by three component:\n",
    "- the `PDEVariationalProblem` `pde` which provides methods for the solution of the forward problem, adjoint problem, and incremental forward and adjoint problems.\n",
    "- the `Prior` `prior` which provides methods to apply the regularization (*precision*) operator to a vector or to apply the prior covariance operator (i.e. to solve linear system with the regularization operator)\n",
    "- the `Misfit` `misfit` which provides methods to compute the cost functional and its partial derivatives with respect to the state and parameter variables.\n",
    "\n",
    "To test gradient and the Hessian of the model we use forward finite differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(pde, prior, misfit)\n",
    "\n",
    "m0 = dl.interpolate(dl.Expression(\"sin(x[0])\", degree=5), Vh[PARAMETER])\n",
    "_ = modelVerify(model, m0.vector(), 1e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compute the MAP point\n",
    "\n",
    "We used the globalized Newtown-CG method to compute the MAP point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = prior.mean.copy()\n",
    "solver = ReducedSpaceNewtonCG(model)\n",
    "solver.parameters[\"rel_tolerance\"] = 1e-6\n",
    "solver.parameters[\"abs_tolerance\"] = 1e-12\n",
    "solver.parameters[\"max_iter\"]      = 25\n",
    "solver.parameters[\"inner_rel_tolerance\"] = 1e-15\n",
    "solver.parameters[\"GN_iter\"] = 5\n",
    "solver.parameters[\"globalization\"] = \"LS\"\n",
    "solver.parameters[\"LS\"][\"c_armijo\"] = 1e-4\n",
    "\n",
    "    \n",
    "x = solver.solve([None, m, None])\n",
    "    \n",
    "if solver.converged:\n",
    "    print( \"\\nConverged in \", solver.it, \" iterations.\")\n",
    "else:\n",
    "    print( \"\\nNot Converged\")\n",
    "\n",
    "print( \"Termination reason: \", solver.termination_reasons[solver.reason] )\n",
    "print( \"Final gradient norm: \", solver.final_grad_norm )\n",
    "print( \"Final cost: \", solver.final_cost )\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "nb.plot(dl.Function(Vh[STATE], x[STATE]), subplot_loc=121,mytitle=\"State\")\n",
    "nb.plot(dl.Function(Vh[PARAMETER], x[PARAMETER]), subplot_loc=122,mytitle=\"Parameter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compute the low rank Gaussian approximation of the posterior\n",
    "We used the *double pass* algorithm to compute a low-rank decomposition of the Hessian Misfit.\n",
    "In particular, we solve\n",
    "\n",
    "$$ \\matHmis {\\bf v}_i = \\lambda_i \\Gprior^{-1} {\\bf v}_i. $$\n",
    "\n",
    "The Figure shows the largest *k* generalized eigenvectors of the Hessian misfit.\n",
    "The effective rank of the Hessian misfit is the number of eigenvalues above the red line ($y=1$).\n",
    "The effective rank is independent of the mesh size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.setPointForHessianEvaluations(x, gauss_newton_approx=False)\n",
    "Hmisfit = ReducedHessian(model, solver.parameters[\"inner_rel_tolerance\"], misfit_only=True)\n",
    "k = 100\n",
    "p = 20\n",
    "print( \"Single/Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p) )\n",
    "\n",
    "Omega = MultiVector(x[PARAMETER], k+p)\n",
    "parRandom.normal(1., Omega)\n",
    "lmbda, V = doublePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k)\n",
    "\n",
    "nu = GaussianLRPosterior(prior, lmbda, V)\n",
    "nu.mean = x[PARAMETER]\n",
    "\n",
    "plt.plot(range(0,k), lmbda, 'b*', range(0,k+1), np.ones(k+1), '-r')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('number')\n",
    "plt.ylabel('eigenvalue')\n",
    "\n",
    "nb.plot_eigenvectors(Vh[PARAMETER], V, mytitle=\"Eigenvector\", which=[0,1,2,5,10,15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prior and LA-posterior pointwise variance fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_trace = True\n",
    "if compute_trace:\n",
    "    post_tr, prior_tr, corr_tr = nu.trace(method=\"Randomized\", r=200)\n",
    "    print( \"LA-Posterior trace {0:5e}; Prior trace {1:5e}; Correction trace {2:5e}\".format(post_tr, prior_tr, corr_tr) )\n",
    "post_pw_variance, pr_pw_variance, corr_pw_variance = nu.pointwise_variance(method=\"Randomized\", r=200)\n",
    "\n",
    "objs = [dl.Function(Vh[PARAMETER], pr_pw_variance),\n",
    "        dl.Function(Vh[PARAMETER], post_pw_variance)]\n",
    "mytitles = [\"Prior variance\", \"LA-Posterior variance\"]\n",
    "nb.multi1_plot(objs, mytitles, logscale=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate samples from Prior and LA-Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 5\n",
    "noise = dl.Vector()\n",
    "nu.init_vector(noise,\"noise\")\n",
    "s_prior = dl.Function(Vh[PARAMETER], name=\"sample_prior\")\n",
    "s_post = dl.Function(Vh[PARAMETER], name=\"sample_post\")\n",
    "\n",
    "pr_max   =  2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max()\n",
    "pr_min   = -2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.min()\n",
    "ps_max   =  2.5*math.sqrt( post_pw_variance.max() ) + nu.mean.max()\n",
    "ps_min   = -2.5*math.sqrt( post_pw_variance.max() ) + nu.mean.min()\n",
    "\n",
    "vmax = max(pr_max, ps_max)\n",
    "vmin = max(pr_min, ps_min)\n",
    "\n",
    "for i in range(nsamples):\n",
    "    parRandom.normal(1., noise)\n",
    "    nu.sample(noise, s_prior.vector(), s_post.vector())\n",
    "    plt.figure(figsize=(15,5))\n",
    "    nb.plot(s_prior, subplot_loc=121,mytitle=\"Prior sample\", vmin=vmin, vmax=vmax)\n",
    "    nb.plot(s_post, subplot_loc=122,mytitle=\"LA-Posterior sample\", vmin=vmin, vmax=vmax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Define a quantify of interest\n",
    "\n",
    "As a quantity of interest, we consider the log of the flux through the bottom boundary:\n",
    "\n",
    "$$ q(m) = \\ln \\left\\{ \\int_{\\Gamma_b} e^m \\nabla u \\cdot \\mathbf{n} \\, ds \\right\\}, $$\n",
    "\n",
    "where the state variable $u$ denotes the pressure, and $\\mathbf{n}$ is the unit normal vector to $\\Gamma_b$ (the bottom boundary of the domain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FluxQOI(object):\n",
    "    def __init__(self, Vh, dsGamma):\n",
    "        self.Vh = Vh\n",
    "        self.dsGamma = dsGamma\n",
    "        self.n = dl.Constant((0.,1.))#dl.FacetNormal(Vh[STATE].mesh())\n",
    "        \n",
    "        self.u = None\n",
    "        self.m = None\n",
    "        self.L = {}\n",
    "        \n",
    "    def form(self, x):\n",
    "        return dl.exp(x[PARAMETER])*dl.dot( dl.grad(x[STATE]), self.n)*self.dsGamma\n",
    "    \n",
    "    def eval(self, x):\n",
    "        u = vector2Function(x[STATE], self.Vh[STATE])\n",
    "        m = vector2Function(x[PARAMETER], self.Vh[PARAMETER])\n",
    "        return np.log( dl.assemble(self.form([u,m])) )\n",
    "\n",
    "class GammaBottom(dl.SubDomain):\n",
    "    def inside(self, x, on_boundary):\n",
    "        return ( abs(x[1]) < dl.DOLFIN_EPS )\n",
    "\n",
    "GC = GammaBottom()\n",
    "marker = dl.FacetFunction(\"size_t\", mesh)\n",
    "marker.set_all(0)\n",
    "GC.mark(marker, 1)\n",
    "dss = dl.Measure(\"ds\", subdomain_data=marker)\n",
    "qoi = FluxQOI(Vh,dss(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Compute posterior expectations using MCMC\n",
    "\n",
    "We compute the mean of the quantity of interest $q$ using MCMC with preconditioned Crank-Nicolson proposal (pCN) and generalized preconditioned Crank-Nicolson proposal (gpCN).\n",
    "\n",
    "### Preconditioned Crank-Nicolson\n",
    "\n",
    "The pCN algorithm is perhaps the simplest MCMC method that is well-defined  in  the  infinite\n",
    "dimensional setting, that is that ensures a mixing rates independent of the dimension of the discretized parameter space.\n",
    "\n",
    "For a given Gaussian prior measure $\\mu_{\\rm prior} \\sim \\mathcal{N}(m_{\\rm prior}, \\mathcal{C}_{\\rm prior})$, a negative log likelihood function $\\Phi(m, \\data) = \\frac{1}{2}\\| {\\bf f}(m) - \\data \\|^2_{\\Gamma_{\\rm noise}^{-1}}$, the acceptance ratio of pCN is defined as\n",
    "\n",
    "$$ a( m_{\\rm current}, m_{\\rm proposed}) := \\min\\left\\{1, \\exp\\left( \\Phi(m_{\\rm current}, \\data) - \\Phi(m_{\\rm proposed}, \\data) \\right) \\right\\}.$$\n",
    "\n",
    "The algorithm below summarizes the pCN method.\n",
    "\n",
    "1. Set $k = 0$ and pick $m^{(0)}$\n",
    "2. Set $v^{(k)} = m_{\\rm prior} + \\sqrt{1 - \\beta^2}(m^{(k)} - m_{\\rm prior}) + \\beta \\xi^{(k)}, \\quad\n",
    "\\xi^{(k)} \\sim \\mathcal{N}( 0, \\mathcal{C}_{\\rm prior} )$\n",
    "3. Set $m^{(k+1)} = v^{(k)}$ with probability $a(m^{(k)}, v^{(k)})$\n",
    "4. Set $m^{(k+1)} = m^{(k)}$ otherwise\n",
    "5. $k \\leftarrow k + 1$ and return to 2\n",
    "\n",
    "Above the parameter $\\beta$ controls the step lenght of the pCN proposals. A small $\\beta$ will lead to a high acceptance ratio, but the proposed sample will be very similar to the current one, thus leading to poor mixing.\n",
    "On the other hand, a too large $\\beta$ will lead to small acceptance ratio, again leading to poor mixing. Therefore, it is important to find the correct trade-off between a large step-size and a good acceptance ratio.\n",
    "\n",
    "\n",
    "### Generalized Preconditioned Crank-Nicolson\n",
    "\n",
    "gpCN is a generalized version of the pCN sampler. While the proposals of pCN are drown from the prior Gaussian distribution $\\mu_{\\rm prior}$, proposals in the generalized pCN are drown from a Gaussian approximation $\\nu$ of the posterior distribution. More specifically, for a given Gaussian prior measure $\\mu_{\\rm prior} \\sim \\mathcal{N}(m_{\\rm prior}, \\mathcal{C}_{\\rm prior})$, a negative log likelihood function $\\Phi(m, \\data) = \\frac{1}{2}\\| {\\bf f}(m) - \\data \\|^2_{\\Gamma_{\\rm noise}^{-1}}$, and a proposal Gaussian distribution $\\nu \\sim \\mathcal{N}(m_\\nu, \\mathcal{C}_\\nu)$, the acceptance ratio of gpCN is defined as\n",
    "\n",
    "$$ a_\\nu( m_{\\rm current}, m_{\\rm proposed}) := \\min\\left\\{1, \\exp\\left( \\Delta(m_{\\rm current}) - \\Delta(m_{\\rm proposed}) \\right) \\right\\}, $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\Delta(m) = \\Phi(m, \\data) + \\frac{1}{2}\\| m - m_{\\rm prior}\\|^2_{\\mathcal{C}_{\\rm prior}^{-1}} - \\frac{1}{2} \\| m - m_\\nu \\|^2_{C_{\\nu}^{-1}}. $$\n",
    "\n",
    "If $\\nu$ is a good Gaussian approximation of $\\mu_{\\rm post}$, one expects $\\Delta$ to be smaller that $\\Phi$, at least in regions of high posterior probability. This suggests that the generalized pCN will have a better acceptance probability than pCN, leading to more rapid sampling.\n",
    "The algorithm below summarizes the gpCN method.\n",
    "\n",
    "1. Set $k = 0$ and pick $m^{(0)}$\n",
    "2. Set $v^{(k)} = m_\\nu + \\sqrt{1 - \\beta^2}(m^{(k)} - m_\\nu) + \\beta \\xi^{(k)}, \\quad\n",
    "\\xi^{(k)} \\sim \\mathcal{N}( 0, \\mathcal{C}_\\nu )$\n",
    "3. Set $m^{(k+1)} = v^{(k)}$ with probability $a_\\nu(m^{(k)}, v^{(k)})$\n",
    "4. Set $m^{(k+1)} = m^{(k)}$ otherwise\n",
    "5. $k \\leftarrow k + 1$ and return to 2\n",
    "\n",
    "> In the code below we ran the chain for 10,000 samples, this is may not be enough to obtain accurate posterior expectation, however it will still give you a feel on how well the chain is mixing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chain(kernel):\n",
    "    noise = dl.Vector()\n",
    "    nu.init_vector(noise, \"noise\")\n",
    "    parRandom.normal(1., noise)\n",
    "    pr_s = model.generate_vector(PARAMETER)\n",
    "    post_s = model.generate_vector(PARAMETER)\n",
    "    # Use a sample from LA-posterior as starting point for the chain\n",
    "    nu.sample(noise, pr_s, post_s, add_mean=True)\n",
    "\n",
    "    chain = MCMC(kernel)\n",
    "    chain.parameters[\"burn_in\"] = 1000\n",
    "    chain.parameters[\"number_of_samples\"] = 10000\n",
    "    chain.parameters[\"print_progress\"] = 10            \n",
    "    tracer = QoiTracer(chain.parameters[\"number_of_samples\"])\n",
    "\n",
    "    n_accept = chain.run(post_s, qoi, tracer)\n",
    "\n",
    "    print( \"Number accepted = {0}\".format(n_accept) )\n",
    "    print( \"E[q] = {0}\".format(chain.sum_q/float(chain.parameters[\"number_of_samples\"])) )\n",
    "    \n",
    "    q = tracer.data\n",
    "    max_lag = 300\n",
    "    integrated_corr_time, lags, acorrs = integratedAutocorrelationTime(q, max_lag)\n",
    "    print (\"Integrated autocorrelation time\", integrated_corr_time)\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(131)\n",
    "    plt.plot(q, '*b')\n",
    "    plt.title(\"Trace plot\")\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.hist(q, normed=True)\n",
    "    plt.title(\"Histogram\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.plot(lags, acorrs, '-b') \n",
    "    plt.title(\"Autocorrelation\")\n",
    "    plt.ylim([0., 1.])\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    return tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sampling using pCN proposal\")\n",
    "kernel_pCN = pCNKernel(model)\n",
    "kernel_pCN.parameters[\"s\"] = 0.0075\n",
    "tracer = run_chain(kernel_pCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sampling using gpCN proposal\")\n",
    "kernel_gpCN = gpCNKernel(model, nu)\n",
    "kernel_gpCN.parameters[\"s\"] = 0.95\n",
    "tracer = run_chain(kernel_gpCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2016-2018, The University of Texas at Austin & University of California, Merced.<br>\n",
    "All Rights reserved.<br>\n",
    "See file COPYRIGHT for details.\n",
    "\n",
    "This file is part of the hIPPYlib library. For more information and source code\n",
    "availability see https://hippylib.github.io.\n",
    "\n",
    "hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
